{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>7SSG2059 Geocomputation 2016/17</h1></center>\n",
    "\n",
    "<h1><center>Practical 10b: Analysis of Relationships in NS-SeC and Values LSOA Data</h1></center>\n",
    "\n",
    "<p><center><i>James Millington, 27 November 2016</i></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Practical 10 is split into two notebooks - one examining relationships in Heathrow Weather and Air Quality data (10a) and one examining relationships in NS-SeC and house prices data (10b). The two notebooks are self-contained and can be used independently. You should decide which sets of data you most likely want to use for your final report, and work through the corresponding notebook during supervised practical time. This will give you the basics of analyses that you can then build on for your final report. Of course, you are welcome to work through both notebooks, although you are unlikely to be able to complete both during class time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Before getting to the data and code in this notebook, you should first run the code in the next three code blocks. These code blocks:\n",
    "\n",
    "1. import packages required for functionality in the remiander of the notebook and set `matplotlib` font parameters \n",
    "2. define a function to help interpret OLS regression output\n",
    "3. define a function to plot a histogram to file\n",
    "\n",
    "Take a quick look at the code when running these blocks, but don't spend too much time as we will return to look at the function definitions more closely later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#import packages required for functionalit below and set matplotlib font parameters \n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sb      \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    #see http://matplotlib.org/users/pyplot_tutorial.html\n",
    "import statsmodels.api as sm       #see http://statsmodels.sourceforge.net/stable/  \n",
    "\n",
    "#set matplotlib font params\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to help interpret OLS regression output\n",
    "def mod_diagnostics(model, data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Output to file model diagnostics for an OLS model\n",
    "    \n",
    "    Input:\n",
    "        model - statsmodels.regression.linear_model.OLS object\n",
    "        data  - pandas.DataFrame containing data for model\n",
    "        \n",
    "    Output:\n",
    "        XX-XX-OLS_SampleXX_Summary.txt contains the model summary output\n",
    "        XX-XX-OLS_SampleXX_ResidHist.png is histogram of the residuals\n",
    "        XX-XX-OLS_SampleXX_StdResid.png is a plot of standardised residuals against fitted values\n",
    "        \n",
    "        if model is univariate: XX-XX_OLS_SampleXX_Regression.png is a scatter plot with regression line\n",
    "        \n",
    "    Requires:\n",
    "        statsmodels.api\n",
    "        pandas\n",
    "        numpy\n",
    "        matplotlib.pyplot\n",
    "    \"\"\"\n",
    "    \n",
    "    fitted = model.fit()\n",
    "    dep = model.endog_names\n",
    "    indep_names = \"\"\n",
    "    \n",
    "    #create a string containing list of indep names for output files\n",
    "    for name in model.exog_names[1:]:            #we don't want 0 element as that is the intercept\n",
    "        indep_names += \"{0}_\".format(name)\n",
    "\n",
    "\n",
    "    #Want to include name of DataFrame in the output filename but currently DataFrame does not have a name attribute\n",
    "    #So for now use nobs from fitted  (Dan potential solution: pass data in a dictionary and access the label)\n",
    "    samplesize = str(int(fitted.nobs))\n",
    "    \n",
    "    f1 = open(\"{0}-{1}OLS_Sample{2}_Summary.txt\".format(dep, indep_names, samplesize), \"w\")\n",
    "    f1.write(fitted.summary().as_text())\n",
    "    f1.close()\n",
    "\n",
    "    #calculate standardized residuals ourselves\n",
    "    fitted_sr = (fitted.resid / np.std(fitted.resid)) \n",
    "\n",
    "    #Histogram of residuals\n",
    "    ax = plt.hist(fitted.resid)\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.savefig('{0}-{1}OLS-Sample{2}_ResidHist.png'.format(dep, indep_names, samplesize), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    #standardized residuals vs fitted values\n",
    "    ax = plt.plot(fitted.fittedvalues, fitted_sr, 'bo')\n",
    "    plt.axhline(linestyle = 'dashed', c = 'black')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    plt.ylabel('Standardized Residuals')                \n",
    "    plt.savefig('{0}-{1}OLS-Sample{2}_StdResid.png'.format(dep, indep_names, samplesize), bbox_inches='tight')\n",
    "    plt.close()\n",
    "  \n",
    "    \n",
    "    if(len(model.exog_names) == 2):  #univariate model (with intercept)\n",
    "            \n",
    "        indep = model.exog_names[1]\n",
    "        \n",
    "        #scatter plot with regression line \n",
    "        ax = plt.plot(data[indep], data[dep], 'bo')\n",
    "        x = np.arange(data[indep].min(), data[indep].max(), 0.1)    #list of values to plot the regression line using\n",
    "        plt.plot(x, fitted.params[1]*x + fitted.params[0], '-', c = 'black')  #plot a line using the standard equation with parms from the model\n",
    "        \n",
    "        plt.xlabel(indep)\n",
    "        plt.ylabel(dep)                \n",
    "        plt.savefig('{0}-{1}OLS_Sample{2}_Regression.png'.format(dep, indep, samplesize), bbox_inches='tight')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to plot histogram to file\n",
    "def plot_hist(series):\n",
    "    \n",
    "    \"\"\"\n",
    "    Output to file a simple histogram\n",
    "    \n",
    "    Input:\n",
    "        series - pandas.Series containing data (may also be able to take a numpy array)\n",
    "        \n",
    "    Output:\n",
    "        XX-SampleXX-Hist.png - the histogram image\n",
    "        \n",
    "    Requires:\n",
    "        pandas\n",
    "        matplotlib.pyplot\n",
    "    \"\"\"\n",
    "\n",
    "    out_name = \"{0}-Sample{1}-Hist.png\".format(series.name, len(series))\n",
    "    plt.hist(series.dropna())\n",
    "    plt.xlabel(series.name)\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(out_name, bbox_inches='tight')           #save the figure\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NS-Sec and Amenity Values\n",
    "\n",
    "The additional data you can use in conjunction with the NS-SeC data are found in `LSOA_ValuesData_London.csv` on KEATS. There are a variety of additional factors that you are free to explore, and you can read about them in the `AdditionalDataOverview.pdf` document also on KEATS. Smith (2010) used similar data in their study which will also likely help you to think about possible analyses you might make for your final report (e.g. between house prices and socio-economic indicators of LSOAs). \n",
    "\n",
    "These data are for housing and other amenity values for LSOAs in London. Consequently, we'll also use only NS-SeC data for London from now on - LSOA NS-SeC data for London only can be found in `Data_NSSHRP_UNIT_URESPOP_London.csv` on KEATS.\n",
    "\n",
    "In Practical 9 we used code to load the two data files for London LSOAs into memory as pandas `dataframes`, tidied up their column names and droped rows with missing data. This code is copied in the next code block - you will need to run this code if you have not already done so to create the `LondonLSOAData.pkl` file. However, if you have already created `LondonLSOAData.pkl` you can skip that code a simply load the data into memory (following code block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##ONLY run this code block if you have NOT already created LondonLSOAData.pkl in Practical 9\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#read NS-SeC data\n",
    "nsCN = [\"CDU_ID\",\"GEO_CODE\",\"GEO_LABEL\",\"F2084\",\"F2085\",\"F2094\",\"F2102\",\"F2107\",\"F2114\",\"F2119\",\"F2127\",\"F2133\",\"F2136\"]  \n",
    "nsDF = pd.read_csv('Data_NSSHRP_UNIT_URESPOP_London.csv', header=0, skiprows=[1], usecols=nsCN)   #read csv with headers, skipping notes row and no data column 15\n",
    "nsDF.columns = [\"CDU_ID\",\"GEO_CODE\",\"GEO_LABEL\",\"Total\",\"Group1\",\"Group2\",\"Group3\",\"Group4\",\"Group5\",\"Group6\",\"Group7\",\"Group8\",\"NC\"]  \n",
    "nsDF = nsDF.dropna(axis = 0)  #drop rows with missing data\n",
    "\n",
    "#read Additional Values Data\n",
    "valCN = [\"lsoa11cd\",\"median_price\",\"avg_distance_to_station\",\"positive_area\",\"moderate_area\",\"negative_area\"]\n",
    "valDF = pd.read_csv('LSOA_ValuesData_London.csv', header=0, usecols = valCN)  \n",
    "valDF = valDF.dropna(axis = 0)  #drop rows with missing data\n",
    "\n",
    "#rename 'lsoa11cd' to 'GEO_CODE'!\n",
    "valDF.columns = [\"GEO_CODE\",\"MedPrice\",\"MeanStationDist\", \"PosArea\", \"ModArea\", \"NegArea\"]  \n",
    "\n",
    "#merge the two data frames \n",
    "nsvalDF = pd.merge(nsDF, valDF, on = 'GEO_CODE')\n",
    "\n",
    "#write data to file\n",
    "nsvalDF.to_csv(\"LondonLSOAData.csv\")\n",
    "nsvalDF.to_pickle(\"LondonLSOAData.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## run this code if you HAVE already created LondonLSOAData.pkl\n",
    "#import pandas as pd                            #already imported above but would be needed otherwise\n",
    "nsvalDF = pd.read_pickle(\"LondonLSOAData.pkl\")  #assumes file is saved in the same folder as this notebook file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the previous code block assumes `LondonLSOAData.pkl` is saved in the same folder as this notebook file, but it is possible to read (and write) data to other folders by specifying the 'path' we want to use. The following code block shows one way to do this (as James discussed in Week 9 lecture). **ONLY** run the next code block if you want to read data from a location other than the folder in which this notebook files is saved - it is more for your information for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##ONLY run this code if you want to read data from a location other than the folder in which this notebook files is saved\n",
    "#import os             #already imported above but would be needed otherwise\n",
    "\n",
    "#set the path to the directory where we want to read and save from\n",
    "path = os.path.join(os.path.expanduser(\"~\"),\"Google Drive\",\"Teaching\",\"2016-17\",\"Undergrad\",\"Geocomp\",\"Week9\")\n",
    "os.chdir(path)\n",
    "\n",
    "#the following line would now read the pkl file from my Week 9 folder (specified above)\n",
    "#nsvalDF.to_pickle(\"LondonLSOAData.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House Price Data\n",
    "\n",
    "As mentioned in lectures, House Price data are often heavily skewed with a long tail (many smaller values and very, very few large values).  \n",
    "\n",
    "Check the distribution of House Price data in the `nsvalDF` DataFrame by creating two plots:\n",
    "\n",
    "1.\tOne plot using the `plot_hist` function defined above (note that this function prints a histogram to an image file on disk)\n",
    "2.\tOne plot using the seaborn `distplot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#your plotting code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from your plots, our House Price data are indeed skewed. When working with House Price data we often used a transformed version of the data that is the logarithm of the original data. One reason is that by reducing the skew of the data we overcome some of the problems of heteroscedasticity but also linear regression models are will better fit more normally distributed data.\n",
    "\n",
    "Let's create a new column in the `nsvalDF` DataFrame that contains the natural logarithm of House Price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nsvalDF['LogMedPrice'] = np.log(nsvalDF[\"MedPrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the new distribution as we just did for the un-transformed data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_hist(nsvalDF[\"LogMedPrice\"])  #uses function defined above (writes to file)\n",
    "\n",
    "sb.distplot(nsvalDF[\"LogMedPrice\"], kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore how house price might be related to the NS-SeC groups let's create jointplots of the logarithm of Median House Price against the population of each NS-SeC group. The code below provides some example code of how you might do this efficiently for all groups with a loop (saving plots to image files on disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "groups = [\"Total\",\"Group1\",\"Group2\",\"Group3\",\"Group4\",\"Group5\",\"Group6\",\"Group7\",\"Group8\"]\n",
    "\n",
    "for group in groups:\n",
    "    \n",
    "    sb.jointplot(nsvalDF['LogMedPrice'], nsvalDF[group])                          \n",
    "    plt.savefig('LogMedPrice_{0}_JointPlot.png'.format(group), bbox_inches='tight')      \n",
    "    plt.close()                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these jointplots we can see there is quite a strong positive relationship between Group1 population and the log-transformed house price data. This seems a good candidate for our first simple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Regression Model\n",
    "To fit a regression we use the `OLS` function in the `statsmodels` package. We can use something like the following (note we imported `statsmodels.api` with alias `sm` above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#create OLS object\n",
    "logMP_G1_mod = sm.OLS.from_formula(\"LogMedPrice ~ Group1\", data = nsvalDF) \n",
    "#fit the regression\n",
    "fitted_logMP_G1_mod = logMP_G1_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how there are two steps to fitting the regression model. First, we create a OLS model object by specifying the 'formula' and the data to use - see that 'formula' does not use the `=` symbol and instead relates variables using `~`. \n",
    "\n",
    "The second line above then actually fits the regression model (using the `fit` method) and puts this in a 'fitted model' object. We can then get a summary of the regression model using the `summary` method with the fitted model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print fitted_logMP_G1_mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "Take a little while to check you understand what is happening in the code above, then answer the following questions from the output of the summary _[edit this text block to answer]_ :\n",
    "\n",
    "Q1:\tHow much variation in LSOA log-transformed median house price is explained by the population of NS-SeC Group 1? \n",
    "\n",
    "**A1:**\n",
    "\n",
    "Q2:\tDoes the confidence interval for the Group 1 parameter value encompass zero?\n",
    "\n",
    "**A2:**\n",
    "\n",
    "Q3:\tDo you think the normality assumption of the residuals has been violated?\n",
    "\n",
    "**A3:**\n",
    "\n",
    "Q4:\tHow many observations were used to fit the model? \n",
    "\n",
    "**A4:**\n",
    "\n",
    "Q5: Given your answer to Q4), do you do you care about your answer to Q3)? _[Hint: think about what Lumley et al. (2002) discuss]_ \n",
    "\n",
    "**A5:**\n",
    "\n",
    "Q6: Replace ??? to characterise the effect size of Group 1 population on Median House Price _Hint: remember the dependent variable is log(price) and read [this](http://stats.stackexchange.com/a/18639) CV answer and see Table 2 of Lin et.al (2013)_\n",
    "\n",
    "**A6:** “For every one person more in Group 1, we would expect median house price to ??? [increase/decrease] by ??? %” \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the number of observations we may not care about whether the residuals are normally distributed but we do still need to check the assumption about heteroscedasticity. You can look at the Durbin-Watson score for this, but given the large number of observations it is probably just as informative to look at plots of the residuals. Rather than type out code to do this we can just use the `mod_diagnostics` function defined for you above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Model Diagnostics\n",
    "\n",
    "Go and look at the `mod_diagnostics` function now to check you understand it. Identify where it does the following:\n",
    "- Fits the model passed to it \n",
    "- Writes the fit model summary to file\n",
    "- Saves the histogram of the residuals to file using a string format\n",
    "- Creates a scatter plot of standardized residuals\n",
    "- Only creates a scatter plot with a regression line when the number of independent variables is equal to 1 (and think about why we don’t try to do this when we have more than one independent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `mod_diagnostics` function to create diagnostic plots for the `logMP_G1_mod` model. Remember two things about `mod_diagnostics` function:\n",
    "1. it takes a OLS model object as an argument, NOT a fitted model object\n",
    "2. it writes its output to file (so you'll need to check your hard disk for output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mod_diagnostics(logMP_G1_mod, nsvalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots you've just created you should be able to see that there is no heteroscedasticity in the residuals. We should be pretty happy with this model!  Remember that by using the `mod_diagnostics` function the summary of the model has been saved to a text file for later consultation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Model\n",
    "\n",
    "Although we can be quite happy with the model we have fit, we should have a think about how we can improve it. For example, how could you improve your statement to answer Q6 above about effect size? Is _absolute_ number of people useful for comparing between LSOAs of differing population size? It would probably be better to consider the number of people in Group 1 as a percentage of the total population of the LSOA. Not only would this make comparison between LSOAs more intuitive but it may also improve the predictive power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task \n",
    "Fit a second simple linear regression to predict log-transformed median house price from the percentage of people in NS-SeC Group 1 in an LSOA. \n",
    "\n",
    "Take the following steps:\n",
    "1.\tCreate a column in `nsvalDF` containing the percentage population of Group1 as a proportion of the Total LSOA population (create the new column like we did for `logMedPrice`)\n",
    "2.\tFit the model using code similar to that provided for you above in the _Fitting a Regression Model_ section\n",
    "3.\tCreate diagnostic plots using the `mod_diagnostics` function (using the code in the _More Model Diagnostics_ section as a guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Use your results from the last task to asnwer the following questions _[edit this text block]_:\n",
    "\n",
    "Q7:\tDoes this model explain more or less variation in log-transformed median house price than the precious model? By how much? \n",
    "\n",
    "**A7: **\n",
    "\n",
    "Q8:\tDo you think this model violates the assumption of about heteroscedasticity of residuals? \n",
    "\n",
    "**A8:  **\n",
    "\n",
    "Q9:\tReplace ??? to explain the effect size:\n",
    "\n",
    "**A9:** “For every one percent more people in Group 1, median house price ??? [increases/decreases] by ??? %”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Model\n",
    "\n",
    "Another way we might improve our model to gain even more explanatory power is to add a second independent variable (making it a multivariate model). For example, maybe we can use the additional Group percentages to improve the model. \n",
    "\n",
    "To do this first we need to create a additional columns in our DataFrame containing Group percentages. The quickest way to do this is a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "groups = [\"Group2\",\"Group3\",\"Group4\",\"Group5\",\"Group6\",\"Group7\",\"Group8\"]\n",
    "for group in groups:\n",
    "    \n",
    "        # Derive Proportions\n",
    "        nsvalDF[group+'Pct'] = 100 * (nsvalDF[group]/nsvalDF[\"Total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including additional predictors is pretty straight-forward to do with the formula syntax of the `statsmodel` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "logP_G1pct_G3pct_mod = sm.OLS.from_formula(\"LogMedPrice ~ Group1Pct + Group3Pct\", data = nsvalDF)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "After fitting the model implied in the last line of code, answer the following questions _[edit this text block]_:\n",
    "\n",
    "Q10: Does the multivariate model improve the amount of variance explained? \n",
    "\n",
    "**A10: **\n",
    "\n",
    "Q11: Is the relationship between Group 3 population and house price positive or negative? Explain why this might make sense. \n",
    "\n",
    "**A11: **\n",
    "\n",
    "Q12: Are you confident both independent variables in this model have an effect on the dependent variable? Why? \n",
    "\n",
    "**A12: **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project\n",
    "\n",
    "Think about what the results above tell us about the how house prices of an area are related to the socio-econmic composition of that area.\n",
    "\n",
    "Start thinking about your final project and what data you might analyse for it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- Lin et al. (2013) Too Big to Fail: Large Samples and the p-Value Problem _Information Systems Research_ 24 906–917 DOI: [10.1287/isre.2013.0480](http://dx.doi.org/10.1287/isre.2013.0480)\n",
    "- Smith, D. (2010) _Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London._ London: Greater London Authority. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "GSA2019",
   "language": "python",
   "name": "gsa2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
